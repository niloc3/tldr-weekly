import json, pathlib, datetime, itertools, collections
from transformers import pipeline

WEEK = 7
TOP_K = 12                # how many stories survive heuristics
OUT = pathlib.Path("site/_posts")  # Jekyll default

today = datetime.date.today()
weekago = today - datetime.timedelta(days=WEEK)

# 1ï¸âƒ£ collect the last 7 days of bulletâ€‘points
stories = []
for day in (weekago + datetime.timedelta(i) for i in range(WEEK)):
    print("ğŸ“…", day)
    print(f"data/{day}")
    for f in pathlib.Path(f"data/{day}").glob("*.json"):
        blob = json.load(open(f))
        stories += [(b["title"], b["link"], b["content"]) for b in blob["items"]]

# 2ï¸âƒ£ very cheap heuristics: score by duplicate URLs & dayâ€‘frequency
score = collections.Counter(url for _, url, _ in stories)
top = sorted(stories, key=lambda t: score[t[1]], reverse=True)[:TOP_K]

print(stories)

# 3ï¸âƒ£ LLM compress titles+blurbs â†’ one weekly summary
pipe = pipeline("summarization", model="mrm8488/t5-base-finetuned-summarize-news")
joined = "\n\n".join(f"{t}\n{c}" for t, _, c in top)
digest = pipe(joined, max_length=230, min_length=120, do_sample=False)[0]["summary_text"]

# 4ï¸âƒ£ write markdown (Jekyll frontâ€‘matter)
fname = OUT / f"{today}.md"
with open(fname, "w") as f:
    f.write(f"---\nlayout: post\ntitle: 'TLDR Weekly Digest â€“Â {today}'\n---\n\n")
    f.write(digest + "\n\n---\n\n")
    for title, url, _ in top:
        f.write(f"* [{title}]({url})\n")
print("ğŸ“ weekly post created:", fname)
